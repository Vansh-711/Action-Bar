# TOOLBOX V2 (VERSION 1.2): ARCHITECTURE REFERENCE
======================================================
Date: February 4, 2026
Evolution: From Linear RAG to Multi-Stage JSON Compilation

---

1. THE PROBLEM WITH V1.1 (WHY WE MOVED)
---------------------------------------
In the previous version (V1.1), the agent was a "Single-Shot Planner". 
- **Rigidity:** It generated one giant JSON plan. If anything changed (hidden Dock, new tab needed), the whole 20-step plan became invalid.
- **Context Loss:** When "Fixing" a step, the AI often forgot the original app (e.g., switching from Brave to Chrome) because it lacked a persistent record of the session.
- **Focus Stealing:** Interacting with the HUD caused macOS menus (Spotlight) to close, breaking the next "Type" action.

---

2. THE V1.2 ARCHITECTURE: THE "6-JSON" PIPELINE
-----------------------------------------------
V1.2 solves these issues by treating AI reasoning as a **Compiled Process** with a physical paper trail (files 1.json to 6.json).

### Stage 1: The Architect (`1_main_breakdown.json`)
- **Action:** Zooms out and breaks the goal into high-level blocks.
- **Output:** A list of task descriptions (e.g. ["Open Browser", "Search Stock"]).
- **Purpose:** Focuses the AI on the "Big Picture" before it worries about mouse clicks.

### Stage 2: Semantic Expansion (`2_semantic_search.json`)
- **Action:** Generates synonyms and related terms for the user goal.
- **Output:** A list of keywords (e.g. "fetch", "stock", "nasdaq").
- **Purpose:** Improves tool retrieval (RAG) by matching patterns, not just exact words.

### Stage 3: Tool Retrieval (`3_available_tools.json`)
- **Action:** Python script queries the Supabase SQL database using keywords from Stage 2.
- **Output:** Full definitions (parameters + body) of matching "LEGO blocks."

### Stage 4: Composition (`4_final_execution.json`)
- **Action:** Combines the Architect's breakdown (Stage 1) and the available Tools (Stage 3).
- **Output:** The actual primitive-level steps (clicks, types, waits).
- **Control:** This plan is reviewed by the user in the HUD before execution starts.

### Stage 5: Surgical Fix (`5_surgical_fix.json`)
- **Action:** Triggered when the user clicks STOP.
- **Context:** Reads the `session_log.txt` (Ground Truth) to see what *really* happened.
- **Output:** A corrective patch for the remaining steps.

### Stage 6: Generalization (`6_generalized_tool.json`)
- **Action:** Post-success optimization.
- **Logic:** Replaces hardcoded names (like "Nvidia") with variables like `{query}`.
- **Result:** Saves the generalized tool to Supabase for all users to benefit.

---

3. KEY TECHNICAL INNOVATIONS
----------------------------

### A. Live System Awareness (`system_monitor.py`)
The agent now has a "Nervous System". Before every prompt, it checks:
- Which app is in front?
- What is the title of the window?
This prevents the AI from trying to "Open Brave" if Brave is already the active window.

### B. Session Log: The "Ground Truth"
We moved away from ephemeral variables. Every action and user feedback is written to `session_log.txt`. 
- Even if the AI's internal state is confused, it can read the text file to see: "The user said Brave didn't open, so I must retry that step."

### C. Surgical Focus Switcher
The HUD now manually executes a `Cmd+Tab` (or `Alt+Tab`) before every block. This ensures that focus is shifted from the HUD back to the work app (Brave/Notes) so keystrokes land in the right place.

### D. Universal Normalizer
Every AI plan is "flattened" and "validated" by Python code before the HUD sees it. If the AI outputs a mess, Python cleans it up into logical LEGO blocks.

---

4. SUMMARY OF IMPROVEMENTS
--------------------------
- **Stability:** 95% (Down from ~60% in V1.1)
- **Learning Speed:** Instant (via generalized tool saves)
- **User Control:** Surgical (Stop/Fix at any tool boundary)
- **Cross-App Data Transfer:** Fully functional via Session Memory.
